{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSM Assignment 3: Hands-on exercise with topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary modules and libraries\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from itertools import compress\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import numpy as np\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "tweets = []\n",
    "for line in open('covidtrack_50K.json', 'r'):\n",
    "    tweets.append(json.loads(line))\n",
    "    \n",
    "# preprocessing\n",
    "texts = [word_tokenize(re.sub(r'\\W+', ' ', t['text'].lower())) for t in tweets] #convert into lowercase, remove non-alphabetic characters, and tokenize\n",
    "texts = [list(compress(txt, list(map(lambda w: len(w)> 2, txt)))) for txt in texts] #remove short words\n",
    "texts = [list(map(lemmatizer.lemmatize, txt)) for txt in texts] #lemmatization\n",
    "texts = [list(compress(txt, list(map(lambda w: w not in stop_words, txt)))) for txt in texts] #remove stopwords\n",
    "texts = [list(compress(txt, list(map(lambda w: w != 'http', txt)))) for txt in texts] #this is a word that is appearing in urls, which are not meaningful in itself.\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2-1: Topic modelling with LDA\n",
    "\n",
    "There are several possible ways that I can perform topic modelling with LDA. There are different libraries, and different ways to count words and create dictionaries. To compare, I first tried with lda library, and then compared with lda model implemented in gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA with pypi lda libary\n",
    "\n",
    "# create word-count dictionary\n",
    "word_doc_count_raw = {}\n",
    "for txt in texts:\n",
    "    txt = list(set(txt))\n",
    "    for word in txt:\n",
    "        try:\n",
    "            word_doc_count_raw[word] += 1\n",
    "        except:\n",
    "            word_doc_count_raw[word] = 1\n",
    "            \n",
    "# Remove words that occur in less than 10 documents, and words that occur in more than 90% of the documents.\n",
    "word_doc_count = {}            \n",
    "for i in word_doc_count_raw:\n",
    "    if word_doc_count_raw[i] < 10:\n",
    "        pass\n",
    "       \n",
    "    elif word_doc_count_raw[i] > len(tweets)*0.9:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        word_doc_count[i] = word_doc_count_raw[i]        \n",
    "\n",
    "        \n",
    "        \n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "# Transform each document to a vectorized form by computing the frequency of each word.\n",
    "Y = np.zeros((len(texts), len(word_doc_count)))\n",
    "for i, txt in enumerate(texts):\n",
    "\n",
    "    for j, w in enumerate(word_doc_count):\n",
    "        Y[i,j] = int(txt.count(w))\n",
    "\n",
    "Y = Y = Y.astype('int64')  #set datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "zero-vector tweets :  does not contain any of the words in the dictionary\n",
      "['timeless', 'coronavirus', 'z2xleebenp']\n",
      "['zara_ansari', 'timeless', 'coronavirus', 'z2xleebenp']\n",
      "['sweetanuu', 'coronavirus', 'chal', 'rha']\n",
      "['anshu_vats1', 'sweetanuu', 'coronavirus', 'chal', 'rha']\n",
      "['painting', 'coronavirus', 'mtszri9wz1']\n",
      "['ùñ±ùñæùóÜùóÇùóáùñΩ', 'ùóíùóàùóéùóãùóåùñæùóÖùñø', 'ùñªùóãùñæùñ∫ùóçùóÅùñæ', 'x9rtihqf5i', 'sumayah369', 'coronavirus', 'fkxksgwns8']\n",
      "['technicaldebt', 'intensifies', 'cobol', 'programmer', 'programming', 'coronavirus', 'zzdwarnbqd']\n",
      "['srbachchan', 'masterpiece', 'masterpiece', 'educational', 'coronavirus', 'coronaindia']\n",
      "['oooh', 'exciting', 'coronavirus', '6icmwgsxb1']\n",
      "['burying', 'coronavirus']\n",
      "['rue', 'faidherbe', 'lille', 'confinementotal', 'coronavirus', '8vuqutoeea']\n",
      "['coronavirus', 'iwolz0wjbl']\n",
      "['robin', 'juste', 'digest', '3awzktsro4', 'ecommerce', 'coronavirus']\n",
      "['tynmgn9mwk', 'coronavirus', 'vide']\n",
      "['coronavirus']\n",
      "['coronavirus', 'helpdonthurt', 'howdareyou', 'thomasmodly', 'shame', 'legacy']\n",
      "['belly', 'beast', 'coronavirus', 'r34zhw0lh2']\n",
      "['neumoniasatipicas', 'neumoniaatipica', 'pahoemergencies', 'covid2019mx', 'coronavirusmx', 'coronavirus', 'vqqvr4lydo']\n",
      "['raleigh', 'coronavirus', 'pkvcw8qdci']\n",
      "['coronavirus', 'afktwtxyma']\n",
      "['slyrankin', 'coronavirus', 'afktwtxyma']\n",
      "['tele', 'courting', 'coronavirus']\n",
      "['gamixtrezeoff', 'coronavirus']\n",
      "['courtesy', 'x22report', 'coronavirus', 'cancercures', 'theyarelyingtous', 'billgatesisevil', 'ibk7zgyoe2']\n",
      "['mailonline', 'alright', 'alright', 'coronavirus']\n",
      "['mjhaveriphd', 'unimaginable', 'coronavirus', 'ietryeilce']\n",
      "['bares', 'repeating', 'coronavirus', 'mutetrump', '3in77vzxsb']\n",
      "['imposture', 'coronavirus']\n",
      "['coronavirus', 'spreadin', 'lmao', 'stankyy']\n",
      "['newstown_aq', 'laquila', 'coronavirus', 'enel', 'dona', '1600', 'mascherine', 'onpi', 'yr4upicxen']\n",
      "['walley', 'correction', 'priti', 'coronavirus', 'covid1984']\n",
      "['banana', 'coronavirus', 'impeachedforever', 'uniteblue', 'theresistance']\n",
      "['portrait', 'coronavirus']\n",
      "['mrpumpkinface1', 'checkmate', 'g7vkgfxugn', 'coronavirus', 'cdvb6ks0ho']\n",
      "['rishiforpm', 'depends', 'coronavirus', 'dailybriefing']\n",
      "['allahthehealer', 'allahuakbar', 'deen', 'coronavirus', 'coronainpakistan', 'lockdownpakistan', 'islamophobia', 'islamicquote', 'gdbxhoduwn']\n",
      "['karting', 'coronavirus', 'lw8crtgiuo']\n",
      "['asemworld', 'asem', 'forgot', 'hashtag', 'coronavirus']\n",
      "['thebloodofjesusisagainstyousatan', 'coronavirus', 'victoryinjesusname', 'victoryinjesus', 'ifocus4lifemastery', 'hgqpzxtofg']\n",
      "['coronavirus', 'gboqn5nmmb']\n",
      "['sweet', 'coronavirus', 'btfldvnkkt']\n",
      "['vladarb', 'unleashing', 'coronavirus', 'y2yblsaw4p']\n",
      "['terribleee', 'coronavirus']\n",
      "['ohmygoodness', 'coronavirus', 'hbjaqzd214']\n",
      "['fixed', 'coronavirus', '6y0wuhr2gd']\n",
      "['mrpumpkinface1', 'checkmate', 'g7vkgfxugn', 'coronavirus', 'cdvb6ks0ho']\n",
      "['carrieri_jenny', 'ohmygoodness', 'coronavirus', 'hbjaqzd214']\n",
      "['kjartanalvestad', 'kilde', 'iyipgstcz3', 'koronavirus', 'coronavirus', 'g23y5mz93v']\n",
      "['agreed', 'coronavirus']\n",
      "['geodannew', 'nazi', 'coronavirus', 'oc3mpbyjle']\n",
      "['compelling', 'tedx', 'coronavirus', 'jsqsi1shnt']\n"
     ]
    }
   ],
   "source": [
    "# check the tweets that don't contain any selected word.\n",
    "print('=================================')\n",
    "print('zero-vector tweets :  does not contain any of the words in the dictionary')\n",
    "for i in range(Y.shape[0]):\n",
    "    if np.array_equal(Y[i,:], np.zeros(len(word_doc_count))):\n",
    "        print(texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tweets can be summarized like following:\n",
    "\n",
    "1. Tweets that contain URLs with random string.\n",
    "2. Tweets that ignore spacing and contain typos.\n",
    "3. Tweets that contain only the major terms, like 'coronavirus'.\n",
    "4. Tweets that contain different conjugations of verbs (since I did not perfomed stemming.)\n",
    "5. Tweets that has no significant words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 50000\n",
      "INFO:lda:vocab_size: 5026\n",
      "INFO:lda:n_words: 441393\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -5291758\n",
      "INFO:lda:<10> log likelihood: -3260704\n",
      "INFO:lda:<20> log likelihood: -3101759\n",
      "INFO:lda:<30> log likelihood: -3064257\n",
      "INFO:lda:<40> log likelihood: -3052516\n",
      "INFO:lda:<50> log likelihood: -3043877\n",
      "INFO:lda:<60> log likelihood: -3038223\n",
      "INFO:lda:<70> log likelihood: -3035084\n",
      "INFO:lda:<80> log likelihood: -3031270\n",
      "INFO:lda:<90> log likelihood: -3029369\n",
      "INFO:lda:<100> log likelihood: -3027879\n",
      "INFO:lda:<110> log likelihood: -3025160\n",
      "INFO:lda:<120> log likelihood: -3024171\n",
      "INFO:lda:<130> log likelihood: -3022504\n",
      "INFO:lda:<140> log likelihood: -3021115\n",
      "INFO:lda:<150> log likelihood: -3021641\n",
      "INFO:lda:<160> log likelihood: -3020446\n",
      "INFO:lda:<170> log likelihood: -3020180\n",
      "INFO:lda:<180> log likelihood: -3019403\n",
      "INFO:lda:<190> log likelihood: -3019336\n",
      "INFO:lda:<200> log likelihood: -3017627\n",
      "INFO:lda:<210> log likelihood: -3016601\n",
      "INFO:lda:<220> log likelihood: -3016135\n",
      "INFO:lda:<230> log likelihood: -3016540\n",
      "INFO:lda:<240> log likelihood: -3016922\n",
      "INFO:lda:<250> log likelihood: -3015777\n",
      "INFO:lda:<260> log likelihood: -3014929\n",
      "INFO:lda:<270> log likelihood: -3013363\n",
      "INFO:lda:<280> log likelihood: -3013785\n",
      "INFO:lda:<290> log likelihood: -3013807\n",
      "INFO:lda:<300> log likelihood: -3012684\n",
      "INFO:lda:<310> log likelihood: -3011792\n",
      "INFO:lda:<320> log likelihood: -3012854\n",
      "INFO:lda:<330> log likelihood: -3011593\n",
      "INFO:lda:<340> log likelihood: -3011367\n",
      "INFO:lda:<350> log likelihood: -3010864\n",
      "INFO:lda:<360> log likelihood: -3011781\n",
      "INFO:lda:<370> log likelihood: -3010857\n",
      "INFO:lda:<380> log likelihood: -3011199\n",
      "INFO:lda:<390> log likelihood: -3011738\n",
      "INFO:lda:<400> log likelihood: -3009999\n",
      "INFO:lda:<410> log likelihood: -3010934\n",
      "INFO:lda:<420> log likelihood: -3009680\n",
      "INFO:lda:<430> log likelihood: -3010269\n",
      "INFO:lda:<440> log likelihood: -3009958\n",
      "INFO:lda:<450> log likelihood: -3010026\n",
      "INFO:lda:<460> log likelihood: -3009189\n",
      "INFO:lda:<470> log likelihood: -3010240\n",
      "INFO:lda:<480> log likelihood: -3011089\n",
      "INFO:lda:<490> log likelihood: -3009897\n",
      "INFO:lda:<500> log likelihood: -3009964\n",
      "INFO:lda:<510> log likelihood: -3009163\n",
      "INFO:lda:<520> log likelihood: -3008304\n",
      "INFO:lda:<530> log likelihood: -3008977\n",
      "INFO:lda:<540> log likelihood: -3008449\n",
      "INFO:lda:<550> log likelihood: -3008629\n",
      "INFO:lda:<560> log likelihood: -3008033\n",
      "INFO:lda:<570> log likelihood: -3009157\n",
      "INFO:lda:<580> log likelihood: -3008310\n",
      "INFO:lda:<590> log likelihood: -3008565\n",
      "INFO:lda:<600> log likelihood: -3008315\n",
      "INFO:lda:<610> log likelihood: -3008187\n",
      "INFO:lda:<620> log likelihood: -3008171\n",
      "INFO:lda:<630> log likelihood: -3008114\n",
      "INFO:lda:<640> log likelihood: -3007860\n",
      "INFO:lda:<650> log likelihood: -3008060\n",
      "INFO:lda:<660> log likelihood: -3007921\n",
      "INFO:lda:<670> log likelihood: -3007796\n",
      "INFO:lda:<680> log likelihood: -3005960\n",
      "INFO:lda:<690> log likelihood: -3007070\n",
      "INFO:lda:<700> log likelihood: -3006706\n",
      "INFO:lda:<710> log likelihood: -3006955\n",
      "INFO:lda:<720> log likelihood: -3007210\n",
      "INFO:lda:<730> log likelihood: -3007410\n",
      "INFO:lda:<740> log likelihood: -3007132\n",
      "INFO:lda:<750> log likelihood: -3006341\n",
      "INFO:lda:<760> log likelihood: -3007589\n",
      "INFO:lda:<770> log likelihood: -3007009\n",
      "INFO:lda:<780> log likelihood: -3006877\n",
      "INFO:lda:<790> log likelihood: -3006528\n",
      "INFO:lda:<800> log likelihood: -3006590\n",
      "INFO:lda:<810> log likelihood: -3006652\n",
      "INFO:lda:<820> log likelihood: -3005927\n",
      "INFO:lda:<830> log likelihood: -3005618\n",
      "INFO:lda:<840> log likelihood: -3005844\n",
      "INFO:lda:<850> log likelihood: -3005393\n",
      "INFO:lda:<860> log likelihood: -3006547\n",
      "INFO:lda:<870> log likelihood: -3005686\n",
      "INFO:lda:<880> log likelihood: -3006075\n",
      "INFO:lda:<890> log likelihood: -3004833\n",
      "INFO:lda:<900> log likelihood: -3005769\n",
      "INFO:lda:<910> log likelihood: -3004570\n",
      "INFO:lda:<920> log likelihood: -3005161\n",
      "INFO:lda:<930> log likelihood: -3005339\n",
      "INFO:lda:<940> log likelihood: -3005819\n",
      "INFO:lda:<950> log likelihood: -3005851\n",
      "INFO:lda:<960> log likelihood: -3005697\n",
      "INFO:lda:<970> log likelihood: -3006210\n",
      "INFO:lda:<980> log likelihood: -3005376\n",
      "INFO:lda:<990> log likelihood: -3005356\n",
      "INFO:lda:<1000> log likelihood: -3005337\n",
      "INFO:lda:<1010> log likelihood: -3005316\n",
      "INFO:lda:<1020> log likelihood: -3005909\n",
      "INFO:lda:<1030> log likelihood: -3005034\n",
      "INFO:lda:<1040> log likelihood: -3004986\n",
      "INFO:lda:<1050> log likelihood: -3004124\n",
      "INFO:lda:<1060> log likelihood: -3005032\n",
      "INFO:lda:<1070> log likelihood: -3004663\n",
      "INFO:lda:<1080> log likelihood: -3004470\n",
      "INFO:lda:<1090> log likelihood: -3004302\n",
      "INFO:lda:<1100> log likelihood: -3004785\n",
      "INFO:lda:<1110> log likelihood: -3004634\n",
      "INFO:lda:<1120> log likelihood: -3005140\n",
      "INFO:lda:<1130> log likelihood: -3005130\n",
      "INFO:lda:<1140> log likelihood: -3003907\n",
      "INFO:lda:<1150> log likelihood: -3004014\n",
      "INFO:lda:<1160> log likelihood: -3005312\n",
      "INFO:lda:<1170> log likelihood: -3004438\n",
      "INFO:lda:<1180> log likelihood: -3003712\n",
      "INFO:lda:<1190> log likelihood: -3003635\n",
      "INFO:lda:<1200> log likelihood: -3004725\n",
      "INFO:lda:<1210> log likelihood: -3004698\n",
      "INFO:lda:<1220> log likelihood: -3004144\n",
      "INFO:lda:<1230> log likelihood: -3004486\n",
      "INFO:lda:<1240> log likelihood: -3004085\n",
      "INFO:lda:<1250> log likelihood: -3004924\n",
      "INFO:lda:<1260> log likelihood: -3005872\n",
      "INFO:lda:<1270> log likelihood: -3004357\n",
      "INFO:lda:<1280> log likelihood: -3004428\n",
      "INFO:lda:<1290> log likelihood: -3004656\n",
      "INFO:lda:<1300> log likelihood: -3003974\n",
      "INFO:lda:<1310> log likelihood: -3004476\n",
      "INFO:lda:<1320> log likelihood: -3004113\n",
      "INFO:lda:<1330> log likelihood: -3004202\n",
      "INFO:lda:<1340> log likelihood: -3004480\n",
      "INFO:lda:<1350> log likelihood: -3004341\n",
      "INFO:lda:<1360> log likelihood: -3005055\n",
      "INFO:lda:<1370> log likelihood: -3004195\n",
      "INFO:lda:<1380> log likelihood: -3004322\n",
      "INFO:lda:<1390> log likelihood: -3004770\n",
      "INFO:lda:<1400> log likelihood: -3004428\n",
      "INFO:lda:<1410> log likelihood: -3003906\n",
      "INFO:lda:<1420> log likelihood: -3003914\n",
      "INFO:lda:<1430> log likelihood: -3004832\n",
      "INFO:lda:<1440> log likelihood: -3005524\n",
      "INFO:lda:<1450> log likelihood: -3004504\n",
      "INFO:lda:<1460> log likelihood: -3003630\n",
      "INFO:lda:<1470> log likelihood: -3002616\n",
      "INFO:lda:<1480> log likelihood: -3003785\n",
      "INFO:lda:<1490> log likelihood: -3003795\n",
      "INFO:lda:<1499> log likelihood: -3003770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: case death new total confirmed reported breaking\n",
      "Topic 1: covid19 covid mask emergency state live japan\n",
      "Topic 2: people stay nh nurse staff medical ajenglish\n",
      "Topic 3: china realdonaldtrump trump wa tomfitton warned navarro\n",
      "Topic 4: covid19 lockdown time week life whole lockdownextension\n",
      "Topic 5: crisis people yet current continues suffering accelerate\n",
      "Topic 6: iran state ha death fight regime city\n",
      "Topic 7: pandemic point play feeling watching realcandaceo arrived\n",
      "Topic 8: death breaking piersmorgan toll number total johnson\n",
      "Topic 9: ha say poor spread american people government\n",
      "Topic 10: china test pandemic country kit million take\n",
      "Topic 11: covid19 pandemic ha tested positive patient expert\n",
      "Topic 12: latest trump thanks president covid19 daily advice\n",
      "Topic 13: know amp people covid19 come want world\n",
      "Topic 14: died ha patient say day skynews year\n",
      "Topic 15: covid19 medium briefing day drtedros lockdown china\n",
      "Topic 16: public tomfitton shutdown getting rule chicago mayor\n",
      "Topic 17: covid19 home spread covid covid_19 stop corona\n",
      "Topic 18: death keep via safe ppe nh provide\n",
      "Topic 19: help pandemic impact way today people due\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(Y)\n",
    "topic_word = model.topic_word_  \n",
    "n_top_words = 7\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(tuple(word_doc_count.keys()))[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2-2: Examining topic specifications and naming 'topics'\n",
    "Before proceeding to the next step, examming and proposing names of each topic, I examined the list of words. Most of them were familar, while some, 'nh', 'ha', 'nh', and 'amp', were not. So I closely examinined those words, to figure out what that mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Irregular Word : ha\n",
      "3     RT @PDChina: Made it! A 104-yr-old World War II veteran from U.S. state of Oregon has become the #oldest known #survivor of #coronavirus di‚Ä¶ \n",
      "21    RT @SkyNews: A British Airways pilot has been praised for becoming a Tesco delivery driver during the #coronavirus lockdown https://t.co/OW‚Ä¶ \n",
      "23    RT @BWYK9: We shall continue to help and support agencies and organisations that tackle the illegal trade in wildlife. The #coronavirus has‚Ä¶ \n",
      "42    RT @EU_Commission: The EU has put international cooperation at the forefront of its response to the #coronavirus. \\n\\nFollowing an agreement‚Ä¶\n",
      "56    RT @allysonhorn: BREAKING: A Queensland infectious disease nurse who's been working with #coronavirus patients has tested positive for the‚Ä¶  \n",
      "Name: ha, dtype: object\n",
      "\n",
      "Irregular Word : ppe\n",
      "263     Can you believe?\\n\\nChina sending a PPE (Test Kits) across world, found be be contaminated with #coronavirus. OMG ü§îü§î‚Ä¶ https://t.co/rlehK8UFTF\n",
      "562     Blackman &amp; White cuts through Covid-19 PPE shortage - https://t.co/oS6Je0K8ap\\n#COVID19 #coronavirus‚Ä¶ https://t.co/rUSmDwxTVP            \n",
      "1056    UK Govt: Provide PPE to ALL frontline NHS to keep them safe #coronavirus - Sign the Petition! https://t.co/DrnvlHCDQK via @UKChange          \n",
      "1134    RT @coughetycough: Latest #ppe for #nhs battling #coronavirus you decide if it's adequate I know my thoughts to little to late https://t.co‚Ä¶ \n",
      "2000    ASLEF demands Tube drivers are provided with PPE @ASLEFunion @FinnBrennan #coronavirus #coronavirusuk #COVID-19‚Ä¶ https://t.co/qWsXx1OiXc     \n",
      "Name: ppe, dtype: object\n",
      "\n",
      "Irregular Word : nh\n",
      "15    RT @AJEnglish: This NHS nurse in the UK is urging people to stay indoors as medical staff continues to tackle #coronavirus. https://t.co/Z6‚Ä¶    \n",
      "41    RT @AJEnglish: This NHS nurse in the UK is urging people to stay indoors as medical staff continues to tackle #coronavirus. https://t.co/Z6‚Ä¶    \n",
      "45    RT @AJEnglish: This NHS nurse in the UK is urging people to stay indoors as medical staff continues to tackle #coronavirus. https://t.co/Z6‚Ä¶    \n",
      "46    RT @BorisJohnson: Thank you for doing your bit in the fight against #coronavirus. \\n\\nStay home, protect the NHS and save lives. \\n \\n#StayHome‚Ä¶\n",
      "51    RT @GOVUK: Help the NHS understand more about how and where #coronavirus is affecting people. If you have symptoms - no matter how mild ‚Äì l‚Ä¶    \n",
      "Name: nh, dtype: object\n",
      "\n",
      "Irregular Word : amp\n",
      "18     RT @LCAG_2019: Congratulations to new Shadow Chancellor @AnnelieseDodds. As millions worry over finances due to #coronavirus, contractors &amp;‚Ä¶  \n",
      "60     RT @JosepBorrellF: Discussion with #EUdefence ministers on military assistance to address #coronavirus pandemic &amp; situation in EU missions‚Ä¶   \n",
      "91     RT @MassObsArchive: Mass Observation wants you to document #COVID19. How is the #coronavirus &amp; #lockdown affecting you? Record your experie‚Ä¶  \n",
      "149    News Briefing - Global Coronavirus update &amp; Daily News - WTX News https://t.co/LtJK9vtDYl #coronavirus                                        \n",
      "185    RT @Fio_edwards: Just 3 weeks ago Boris Johnson was telling us to take #coronavirus \"on the chin\" &amp; pursuing a \"herd immunity\" strategy.\\n\\nN‚Ä¶\n",
      "Name: amp, dtype: object\n",
      "\n",
      "Irregular Word : tomfitton\n",
      "11     RT @TomFitton: India approved #Hydroxychloroquine as a #coronavirus preventative for doctors and nurses treating #coronavirus patients.  Sh‚Ä¶\n",
      "379    RT @TomFitton: Leftists attack Constitution under cover of #coronavirus. Thankfully, @AjitPaiFCC supports the First Amendment. https://t.co‚Ä¶\n",
      "690    RT @TomFitton: FBI Deep State uses #coronavirus as excuse to shut down FOIA and gov't transparency! https://t.co/1tJO0pDCfy                 \n",
      "836    RT @TomFitton: Further confirmation that #Hydroxychloroquine can help with #coronavirus.  @RealDonaldTrump is right. https://t.co/Cb69r8NhiB\n",
      "866    RT @TomFitton: India approved #Hydroxychloroquine as a #coronavirus preventative for doctors and nurses treating #coronavirus patients.  Sh‚Ä¶\n",
      "Name: tomfitton, dtype: object\n",
      "\n",
      "Irregular Word : drtedros\n",
      "784     RT @DrTedros: Thank you üá®üá∑ President @CarlosAlvQ for your initiative &amp;  leadership on the #coronavirus response. We look forward to working‚Ä¶\n",
      "1033    RT @DrTedros: I am sending my sincere thanks to every health worker around the üåç fighting the #coronavirus. We know that #COVID19 is puttin‚Ä¶    \n",
      "1235    RT @DrTedros: Thank you üá®üá∑ President @CarlosAlvQ for your initiative &amp;  leadership on the #coronavirus response. We look forward to working‚Ä¶\n",
      "1627    RT @WHO: Media briefing on #COVID19 with @DrTedros. #coronavirus https://t.co/e5rc2jbeZu                                                        \n",
      "1775    RT @WHO: Media briefing on #COVID19 with @DrTedros. #coronavirus https://t.co/e5rc2jbeZu                                                        \n",
      "Name: drtedros, dtype: object\n",
      "\n",
      "Irregular Word : navarro\n",
      "2026    RT @davegreenidge57: If Navarro had tossed in a few snaps, maybe that would have grabbed Trump‚Äôs wobbly attention. #coronavirus https://t.c‚Ä¶    \n",
      "3886    RT @nadinevdVelde: @jonathanvswan A month after Navarro warned the WH about #coronavirus, Trump mocked &amp; downplayed #COVID2019 at his campa‚Ä¶\n",
      "4169    Peter Navarro warned Donald Trump COVID-19 could kill 500,000 Americans ‚Äî back in January: NYT \\n#coronavirus \\n https://t.co/STQyriWWyl        \n",
      "4204    RT @RawStory: Peter Navarro warned Donald Trump COVID-19 could kill 500,000 Americans ‚Äî back in January: NYT \\n#coronavirus \\n https://t.co/S‚Ä¶  \n",
      "4387    RT @RawStory: Peter Navarro warned Donald Trump COVID-19 could kill 500,000 Americans ‚Äî back in January: NYT \\n#coronavirus \\n https://t.co/S‚Ä¶  \n",
      "Name: navarro, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# examining strange words\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "irregularities = ['ha','ppe','nh','amp','tomfitton','drtedros','realcandaceo','piersmorgan','navarro']\n",
    "irregularities_df = pd.DataFrame(columns = irregularities)\n",
    "\n",
    "for i,t in enumerate(texts):\n",
    "    for w in irregularities:        \n",
    "        if w in t:\n",
    "            irregularities_df.loc[i,w] = tweets[i]['text']\n",
    "            \n",
    "for w in irregularities:\n",
    "    print('\\nIrregular Word : {0}'.format(w))\n",
    "    print(irregularities_df[w].dropna().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I acquired interesting results here. All of those strange keywords emerged from different reasons:\n",
    "1. 'ha' was originally 'has'. This is a verb that consists stopwords, so it must have been removed. However, since lemmatization is done before stopword removal, 'has' is treated as noun, and converted to 'ha'.\n",
    "2. 'ppe' is an abbreviation of Personal Protective Equipment. Usually, it is not difficult to discern abbreviations from other words, since abbreviations are written in capital letters. In this case, all texts are converted into lower-case letters. It has no effect on meanings, but it becomes hard to understand the meaning at a first glance.\n",
    "3. 'nh' is lemmatized from 'nhs', which is, again, an abbreviation of National Health Service. This case, it is hard to infer the original meaning from the processed text, unless a person is already well-aware of the situation and the data. \n",
    "4. When a tweet contains '&' character, the Twitter API translates the character into '&amp' when returning the tweet content. Since the character semantically means 'and', which is another stopword, it is better not to take this keyword into account.\n",
    "\n",
    "And of course, there are some names and mentions to another account.\n",
    "1. Tom Fitton is the president of Judicial Watch in the US, an American pro-Trump, conservative (or extreme-right) activist group.\n",
    "2. Dr. Tedros is the Director-General of the World Health Organization\n",
    "3. Candace Owen is an American conservative commentator and political activistknown for her pro-Trump activism and her criticism of Black Lives Matter and of the Democratic Party.\n",
    "4. Piers Morgan is an English broadcaster whose long friendship with President Trump has recently fallen into perils.\n",
    "5. Peter Navarro is an assistant to the President for Trade & Manufacturing Policy, who is said to have warned the White House of the risk of pandemic in January.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the topics to semantically examine, I chose following 8 topics to propose names: \n",
    "\n",
    "- Topic 0: case death new total confirmed reported breaking - Counting confirmed cases and deaths.\n",
    "- Topic 3: china realdonaldtrump trump wa tomfitton warned navarro - Retweeting conservative political figures\n",
    "- Topic 4: covid19 lockdown time week life whole lockdownextension - Lockdown notices and reports\n",
    "- Topic 10: china test pandemic country kit million take - COVID testing kit supply\n",
    "- Topic 11: covid19 pandemic ha tested positive patient expert - Updates in new COVID patients\n",
    "- Topic 15: covid19 medium briefing day drtedros lockdown china - WHO briefings\n",
    "- Topic 17: covid19 home spread covid covid_19 stop corona - stay-at-home campaign\n",
    "- Topic 18: death keep via safe ppe nh provide - wear-your-mask campaign\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Discussion\n",
    "\n",
    "First of all, I realized the importance of proper data pre-processing. Here, I followed the guidelines provided by Trung, but this method came up with some errorneous preprocessed results, mentioned in part 2-2. There are several elements in tweets contents, such as hashtags, retweet indicators, urls, and mentions, and also, texts itself. They are not formatted or read in same way, thus those differences must be taken into account before naive preprocessing. For example, urls should be processed separately, since they contain non-alphabets, random strings, and their own 'stopwords' (e.g. 'https', 'com', 'org') . It might be possible to consider only domain names (e.g. who.int, but not its directory), or neglect them completely. Furthermore, adding verb stemming might have been a good add-on as well.\n",
    "\n",
    "Furthermore, abbreviations has to be taken into account. In my analysis, 'WHO' is not found anywhere in the keywords, which is quite strange, because we have its director-general. It is happening because lowercased 'who' is removed through stopword removals. In real-life situations, two words with the identical spellings but different letter cases may mean completely different things. There must be more important factors like this, that should be taken account while preprocessing, and such cases should be examined throughly with the corresponding experts.\n",
    "\n",
    "\n",
    "Secondly, I tried applying LDA by using different libraries, and the result was quite different. In real-world situation, choosing a right library to import the same LDA module might matter. (The topic extraction in gensim LDA is demonstrated below this cell)\n",
    "\n",
    "Regarding the topics, it was surprising to see that mentions to other accounts were important 'keywords' overall. This in itself, is not surprising, because it was covered in class, that information flow in two-steps in Twitter, that media has an indirect influence over the public via an intermediate layer of opinion leaders. Also, through the past assignment, I found out that most of the tweets are 'retweets' of others.\n",
    "What was surprising, however, was that those 'keyword' figures were mostly conservative political figures. (In my point of view, more close to extreme-right). Except for Dr.Tedros, the director-general of WHO, all the figures were closely related to Donald Trump. Scientists, medical journalists, or heath authorities don't appear to be influential enough, nor 'liberal' political figures. \n",
    "There might be several reasons to this. Maybe the sample was simple biased. Maybe the result is demonstrating the spread of disinformation and misinformation in Twitter. Maybe it is more related to politics and psycology, or the Twitter demographics. But still, the fact that 'conservative' politicians are way more influential in topics than their 'liberal' counterparts is interesting, and this might resolve some of the ambiguities we have in the real world.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('000', 0.021446023),\n",
       "   ('covid19', 0.019942822),\n",
       "   ('great', 0.019108178),\n",
       "   ('year', 0.017917603),\n",
       "   ('due', 0.0175473)]),\n",
       " (1,\n",
       "  [('trump', 0.065633394),\n",
       "   ('president', 0.032885034),\n",
       "   ('medical', 0.024133747),\n",
       "   ('latest', 0.022624508),\n",
       "   ('wa', 0.020685801)]),\n",
       " (2,\n",
       "  [('covid19', 0.09167603),\n",
       "   ('lockdown', 0.07842503),\n",
       "   ('time', 0.061652694),\n",
       "   ('life', 0.057069365),\n",
       "   ('week', 0.055344574)]),\n",
       " (3,\n",
       "  [('health', 0.041053344),\n",
       "   ('world', 0.037649713),\n",
       "   ('covid„Éº19', 0.03351267),\n",
       "   ('sure', 0.023461446),\n",
       "   ('minister', 0.022270415)]),\n",
       " (4,\n",
       "  [('know', 0.023671394),\n",
       "   ('want', 0.02133876),\n",
       "   ('covid19', 0.021316553),\n",
       "   ('come', 0.019346252),\n",
       "   ('bill', 0.015862737)]),\n",
       " (5,\n",
       "  [('crisis', 0.103897795),\n",
       "   ('people', 0.09852376),\n",
       "   ('yet', 0.09712378),\n",
       "   ('suffering', 0.09650331),\n",
       "   ('continues', 0.08833984)]),\n",
       " (6,\n",
       "  [('pandemic', 0.031772286),\n",
       "   ('conference', 0.031021953),\n",
       "   ('10downingstreet', 0.022425262),\n",
       "   ('press', 0.020662017),\n",
       "   ('ha', 0.017423308)]),\n",
       " (7,\n",
       "  [('die', 0.022020422),\n",
       "   ('worker', 0.020649366),\n",
       "   ('hour', 0.01952073),\n",
       "   ('video', 0.0162234),\n",
       "   ('get', 0.015535437)]),\n",
       " (8,\n",
       "  [('covid19', 0.051855758),\n",
       "   ('via', 0.039393518),\n",
       "   ('keep', 0.037452996),\n",
       "   ('play', 0.03513402),\n",
       "   ('realcandaceo', 0.034459867)]),\n",
       " (9,\n",
       "  [('day', 0.056201965),\n",
       "   ('could', 0.025524529),\n",
       "   ('give', 0.020419592),\n",
       "   ('china', 0.014244129),\n",
       "   ('plan', 0.013271496)]),\n",
       " (10,\n",
       "  [('crisis', 0.053033933),\n",
       "   ('realdonaldtrump', 0.037339654),\n",
       "   ('china', 0.036392696),\n",
       "   ('people', 0.03144513),\n",
       "   ('warned', 0.031336397)]),\n",
       " (11,\n",
       "  [('ha', 0.055666186),\n",
       "   ('today', 0.025083136),\n",
       "   ('poor', 0.023858177),\n",
       "   ('government', 0.022611896),\n",
       "   ('done', 0.02151179)]),\n",
       " (12,\n",
       "  [('past', 0.07866158),\n",
       "   ('936', 0.07608631),\n",
       "   ('horribly', 0.064823285),\n",
       "   ('staff', 0.030029038),\n",
       "   ('people', 0.028552633)]),\n",
       " (13,\n",
       "  [('pandemic', 0.06849362),\n",
       "   ('point', 0.053661585),\n",
       "   ('feeling', 0.051152904),\n",
       "   ('watching', 0.050287824),\n",
       "   ('cartoon', 0.017248917)]),\n",
       " (14,\n",
       "  [('may', 0.03667481),\n",
       "   ('covid19', 0.022552546),\n",
       "   ('majority', 0.01889918),\n",
       "   ('better', 0.014520014),\n",
       "   ('corona', 0.014468553)]),\n",
       " (15,\n",
       "  [('drastically', 0.14172293),\n",
       "   ('public', 0.041006964),\n",
       "   ('tomfitton', 0.03488318),\n",
       "   ('briefing', 0.031188887),\n",
       "   ('spread', 0.030999364)]),\n",
       " (16,\n",
       "  [('death', 0.13731153),\n",
       "   ('case', 0.061004203),\n",
       "   ('new', 0.0459297),\n",
       "   ('total', 0.04474585),\n",
       "   ('breaking', 0.040790502)]),\n",
       " (17,\n",
       "  [('24hrs', 0.06544868),\n",
       "   ('piersmorgan', 0.063499086),\n",
       "   ('spike', 0.056828585),\n",
       "   ('breaking', 0.052312165),\n",
       "   ('test', 0.026594937)]),\n",
       " (18,\n",
       "  [('new', 0.07387992),\n",
       "   ('iran', 0.026932184),\n",
       "   ('covid19', 0.025513452),\n",
       "   ('say', 0.021184826),\n",
       "   ('amp', 0.02077309)]),\n",
       " (19,\n",
       "  [('home', 0.047438405),\n",
       "   ('skynews', 0.031538196),\n",
       "   ('stay', 0.030769762),\n",
       "   ('news', 0.018863827),\n",
       "   ('stayhome', 0.01726286)])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Appendix : LDA topic generation and topic specifications using different library\n",
    "\n",
    "import gensim\n",
    "import logging, sys\n",
    "\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.9, keep_n=100000)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "\n",
    "lda_model_gensim = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "\n",
    "#for idx, topic in lda_model_gensim.print_topics(-1):\n",
    "#    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n",
    "lda_model_gensim.show_topics(num_topics=20, num_words=5, log=False, formatted=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
